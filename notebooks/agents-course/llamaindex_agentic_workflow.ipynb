{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows in LlamaIndex\n",
    "\n",
    "\n",
    "This notebook is part of the [Hugging Face Agents Course](https://www.hf.co/learn/agents-course), a free Course from beginner to expert, where you learn to build Agents.\n",
    "\n",
    "![Agents course share](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png)\n",
    "\n",
    "## Let's install the dependencies\n",
    "\n",
    "We will install the dependencies for this unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index datasets llama-index-callbacks-arize-phoenix llama-index-vector-stores-chroma llama-index-utils-workflow llama-index-llms-huggingface-api pyvis -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting llama-index==0.10.40\n",
      "  Downloading llama_index-0.10.40-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.40 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_core-0.10.68.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_llms_openai-0.1.31-py3-none-any.whl.metadata (650 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_readers_file-0.1.33-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index==0.10.40)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: openai>=1.14.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index==0.10.40) (1.66.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2024.9.0)\n",
      "Requirement already satisfied: httpx in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (3.4.2)\n",
      "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2.2.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (11.0.0)\n",
      "Requirement already satisfied: pydantic<3.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.14.1)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.19 (from llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.40) (4.13.3)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.40)\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.40) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.4.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40) (0.6.4.post1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.40) (2.6)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.4 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-parse>=0.4.0->llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40) (0.6.6)\n",
      "Requirement already satisfied: anyio in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (0.14.0)\n",
      "Requirement already satisfied: click in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2024.11.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index==0.10.40) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index==0.10.40) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index==0.10.40) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from pydantic<3.0->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from pydantic<3.0->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (3.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (2025.1)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.15 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from llama-cloud-services>=0.6.4->llama-parse>=0.4.0->llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40) (0.1.15)\n",
      "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.4.0->llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_cloud_services-0.6.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting llama-cloud<0.2.0,>=0.1.16 (from llama-cloud-services>=0.6.4->llama-parse>=0.4.0->llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_cloud-0.1.16-py3-none-any.whl.metadata (902 bytes)\n",
      "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.4.0->llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_cloud_services-0.6.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading llama_cloud_services-0.6.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading llama_parse-0.6.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.3 (from llama-parse>=0.4.0->llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_cloud_services-0.6.3-py3-none-any.whl.metadata (2.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_parse-0.6.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.2 (from llama-parse>=0.4.0->llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_cloud_services-0.6.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_parse-0.6.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.1 (from llama-parse>=0.4.0->llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_cloud_services-0.6.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_parse-0.6.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting llama-cloud-services (from llama-parse>=0.4.0->llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_cloud_services-0.6.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.40)\n",
      "  Downloading llama_parse-0.5.20-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.18-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.17-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.16-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.15-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.14-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.13-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading llama_parse-0.5.12-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.11-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.10-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.9-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.8-py3-none-any.whl.metadata (6.4 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading llama_parse-0.5.7-py3-none-any.whl.metadata (6.4 kB)\n",
      "  Downloading llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.5-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "  Downloading llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/nurbot/miniconda3/envs/smol/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.40->llama-index==0.10.40) (1.17.0)\n",
      "Downloading llama_index-0.10.40-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.10.68.post1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
      "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.1.31-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.1.7-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.33-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
      "Downloading llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
      "Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
      "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: tenacity, pypdf, llamaindex-py-client, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "  Attempting uninstall: pypdf\n",
      "    Found existing installation: pypdf 5.4.0\n",
      "    Uninstalling pypdf-5.4.0:\n",
      "      Successfully uninstalled pypdf-5.4.0\n",
      "  Attempting uninstall: llama-index-core\n",
      "    Found existing installation: llama-index-core 0.12.25\n",
      "    Uninstalling llama-index-core-0.12.25:\n",
      "      Successfully uninstalled llama-index-core-0.12.25\n",
      "  Attempting uninstall: llama-parse\n",
      "    Found existing installation: llama-parse 0.6.4.post1\n",
      "    Uninstalling llama-parse-0.6.4.post1:\n",
      "      Successfully uninstalled llama-parse-0.6.4.post1\n",
      "  Attempting uninstall: llama-index-readers-file\n",
      "    Found existing installation: llama-index-readers-file 0.4.6\n",
      "    Uninstalling llama-index-readers-file-0.4.6:\n",
      "      Successfully uninstalled llama-index-readers-file-0.4.6\n",
      "  Attempting uninstall: llama-index-llms-openai\n",
      "    Found existing installation: llama-index-llms-openai 0.3.26\n",
      "    Uninstalling llama-index-llms-openai-0.3.26:\n",
      "      Successfully uninstalled llama-index-llms-openai-0.3.26\n",
      "  Attempting uninstall: llama-index-indices-managed-llama-cloud\n",
      "    Found existing installation: llama-index-indices-managed-llama-cloud 0.6.9\n",
      "    Uninstalling llama-index-indices-managed-llama-cloud-0.6.9:\n",
      "      Successfully uninstalled llama-index-indices-managed-llama-cloud-0.6.9\n",
      "  Attempting uninstall: llama-index-embeddings-openai\n",
      "    Found existing installation: llama-index-embeddings-openai 0.3.1\n",
      "    Uninstalling llama-index-embeddings-openai-0.3.1:\n",
      "      Successfully uninstalled llama-index-embeddings-openai-0.3.1\n",
      "  Attempting uninstall: llama-index-readers-llama-parse\n",
      "    Found existing installation: llama-index-readers-llama-parse 0.4.0\n",
      "    Uninstalling llama-index-readers-llama-parse-0.4.0:\n",
      "      Successfully uninstalled llama-index-readers-llama-parse-0.4.0\n",
      "  Attempting uninstall: llama-index-multi-modal-llms-openai\n",
      "    Found existing installation: llama-index-multi-modal-llms-openai 0.4.3\n",
      "    Uninstalling llama-index-multi-modal-llms-openai-0.4.3:\n",
      "      Successfully uninstalled llama-index-multi-modal-llms-openai-0.4.3\n",
      "  Attempting uninstall: llama-index-cli\n",
      "    Found existing installation: llama-index-cli 0.4.1\n",
      "    Uninstalling llama-index-cli-0.4.1:\n",
      "      Successfully uninstalled llama-index-cli-0.4.1\n",
      "  Attempting uninstall: llama-index-agent-openai\n",
      "    Found existing installation: llama-index-agent-openai 0.4.6\n",
      "    Uninstalling llama-index-agent-openai-0.4.6:\n",
      "      Successfully uninstalled llama-index-agent-openai-0.4.6\n",
      "  Attempting uninstall: llama-index-program-openai\n",
      "    Found existing installation: llama-index-program-openai 0.3.1\n",
      "    Uninstalling llama-index-program-openai-0.3.1:\n",
      "      Successfully uninstalled llama-index-program-openai-0.3.1\n",
      "  Attempting uninstall: llama-index-question-gen-openai\n",
      "    Found existing installation: llama-index-question-gen-openai 0.3.0\n",
      "    Uninstalling llama-index-question-gen-openai-0.3.0:\n",
      "      Successfully uninstalled llama-index-question-gen-openai-0.3.0\n",
      "  Attempting uninstall: llama-index\n",
      "    Found existing installation: llama-index 0.12.25\n",
      "    Uninstalling llama-index-0.12.25:\n",
      "      Successfully uninstalled llama-index-0.12.25\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-cloud-services 0.6.6 requires llama-index-core>=0.11.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-embeddings-huggingface-api 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-utils-huggingface 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-vector-stores-chroma 0.4.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-embeddings-huggingface 0.5.2 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-callbacks-arize-phoenix 0.4.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-tools-google 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-utils-workflow 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-llms-huggingface-api 0.4.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed llama-index-0.10.40 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-core-0.10.68.post1 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.1.31 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.7 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.33 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.9 llamaindex-py-client-0.1.19 pypdf-4.3.1 tenacity-8.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install llama-index==0.10.40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, let's log in to Hugging Face to use serverless Inference APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d13e1ae4f984563b75dcc521f468d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Workflow Creation\n",
    "\n",
    "We can start by creating a simple workflow. We use the `StartEvent` and `StopEvent` classes to define the start and stop of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
    "\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something here\n",
    "        return StopEvent(result=\"Hello, world!\")\n",
    "\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Multiple Steps\n",
    "\n",
    "We can also create multi-step workflows. Here we pass the event information between steps. Note that we can use type hinting to specify the event type and the flow of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finished processing: Step 1 complete'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops and Branches\n",
    "\n",
    "We can also use type hinting to create branches and loops. Note that we can use the `|` operator to specify that the step can return multiple types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good thing happened\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: First step complete.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "import random\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_output: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Bad thing happened\")\n",
    "            return LoopEvent(loop_output=\"Back to step one.\")\n",
    "        else:\n",
    "            print(\"Good thing happened\")\n",
    "            return ProcessingEvent(intermediate_result=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing Workflows\n",
    "\n",
    "We can also draw workflows using the `draw_all_possible_flows` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class '__main__.ProcessingEvent'>\n",
      "<class '__main__.LoopEvent'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "workflow_all_flows.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![drawing](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflow-draw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Management\n",
    "\n",
    "Instead of passing the event information between steps, we can use the `Context` type hint to pass information between steps. \n",
    "This might be useful for long running workflows, where you want to store information between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the capital of France?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: Step 1 complete'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event, Context\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent, ctx: Context) -> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        await ctx.set(\"query\", \"What is the capital of France?\")\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent, ctx: Context) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        query = await ctx.get(\"query\")\n",
    "        print(f\"Query: {query}\")\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent Workflows\n",
    "\n",
    "We can also create multi-agent workflows. Here we define two agents, one that multiplies two integers and one that adds two integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AgentWorkflow' from 'llama_index.agent' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceInferenceAPI\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentWorkflow\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define some tools\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd\u001b[39m(a: \u001b[38;5;28mint\u001b[39m, b: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AgentWorkflow' from 'llama_index.agent' (unknown location)"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.agent.workflow import AgentWorkflow\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
    "multiply_agent = ReActAgent(\n",
    "    name=\"multiply_agent\",\n",
    "    description=\"Is able to multiply two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
    "    tools=[multiply], \n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "addition_agent = ReActAgent(\n",
    "    name=\"add_agent\",\n",
    "    description=\"Is able to add two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
    "    tools=[add], \n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Create the workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\"\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
