{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off\n",
    "!pip install -qqq flash-attn --no-build-isolation --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 20:45:54.431845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742067954.444258 1662000 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742067954.448190 1662000 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742067954.458729 1662000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742067954.458742 1662000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742067954.458743 1662000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742067954.458745 1662000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-15 20:45:54.462193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtobrun\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"mlabonne/smoltldr\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,884,480 || all params: 139,399,488 || trainable%: 3.5039\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function\n",
    "ideal_length = 50\n",
    "\n",
    "\n",
    "def reward_len(completions, **kwargs):\n",
    "    return [-abs(ideal_length - len(completion)) for completion in completions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"GRPO\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_prompt_length=512,\n",
    "    max_completion_length=96,\n",
    "    num_generations=8,\n",
    "    optim=\"adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nurbot/ws/gist/notebooks/wandb/run-20250315_204602-tl4ltwyb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobrun/GRPO/runs/tl4ltwyb' target=\"_blank\">quiet-wave-2</a></strong> to <a href='https://wandb.ai/tobrun/GRPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tobrun/GRPO' target=\"_blank\">https://wandb.ai/tobrun/GRPO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tobrun/GRPO/runs/tl4ltwyb' target=\"_blank\">https://wandb.ai/tobrun/GRPO/runs/tl4ltwyb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8fd063b3214b1a9ad9735680de861b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.2826915681362152, 'learning_rate': 1.9840000000000003e-05, 'completion_length': 83.046875, 'rewards/reward_len': -274.4765625, 'reward': -274.4765625, 'reward_std': 87.35344696044922, 'kl': 0.0, 'epoch': 0.01}\n",
      "{'loss': 0.0001, 'grad_norm': 0.299862802028656, 'learning_rate': 1.968e-05, 'completion_length': 88.84375, 'rewards/reward_len': -296.578125, 'reward': -296.578125, 'reward_std': 68.57475662231445, 'kl': 0.0014800804783590138, 'epoch': 0.02}\n",
      "{'loss': 0.0001, 'grad_norm': 0.21764428913593292, 'learning_rate': 1.9520000000000003e-05, 'completion_length': 88.515625, 'rewards/reward_len': -296.109375, 'reward': -296.109375, 'reward_std': 72.1087417602539, 'kl': 0.001574683585204184, 'epoch': 0.02}\n",
      "{'loss': 0.0001, 'grad_norm': 0.22813670337200165, 'learning_rate': 1.936e-05, 'completion_length': 84.34375, 'rewards/reward_len': -273.84375, 'reward': -273.84375, 'reward_std': 77.6366195678711, 'kl': 0.0015627023531123996, 'epoch': 0.03}\n",
      "{'loss': 0.0001, 'grad_norm': 0.19130122661590576, 'learning_rate': 1.9200000000000003e-05, 'completion_length': 88.2109375, 'rewards/reward_len': -286.2890625, 'reward': -286.2890625, 'reward_std': 69.31764221191406, 'kl': 0.0015593686839565635, 'epoch': 0.04}\n",
      "{'loss': 0.0001, 'grad_norm': 0.23323260247707367, 'learning_rate': 1.904e-05, 'completion_length': 87.2265625, 'rewards/reward_len': -286.359375, 'reward': -286.359375, 'reward_std': 75.01750946044922, 'kl': 0.0016035157605074346, 'epoch': 0.05}\n",
      "{'loss': 0.0001, 'grad_norm': 0.2162223756313324, 'learning_rate': 1.8880000000000002e-05, 'completion_length': 84.5078125, 'rewards/reward_len': -272.875, 'reward': -272.875, 'reward_std': 67.22573852539062, 'kl': 0.0016493410803377628, 'epoch': 0.06}\n",
      "{'loss': 0.0001, 'grad_norm': 0.2967580258846283, 'learning_rate': 1.8720000000000004e-05, 'completion_length': 85.84375, 'rewards/reward_len': -282.0234375, 'reward': -282.0234375, 'reward_std': 79.65451049804688, 'kl': 0.0016613301122561097, 'epoch': 0.06}\n",
      "{'loss': 0.0001, 'grad_norm': 0.2936781048774719, 'learning_rate': 1.8560000000000002e-05, 'completion_length': 85.5234375, 'rewards/reward_len': -277.8515625, 'reward': -277.8515625, 'reward_std': 81.67067337036133, 'kl': 0.0017264700145460665, 'epoch': 0.07}\n",
      "{'loss': 0.0001, 'grad_norm': 0.2377161979675293, 'learning_rate': 1.8400000000000003e-05, 'completion_length': 84.4609375, 'rewards/reward_len': -271.78125, 'reward': -271.78125, 'reward_std': 75.76887512207031, 'kl': 0.0017107015009969473, 'epoch': 0.08}\n",
      "{'loss': 0.0001, 'grad_norm': 0.21083952486515045, 'learning_rate': 1.824e-05, 'completion_length': 89.1484375, 'rewards/reward_len': -299.0390625, 'reward': -299.0390625, 'reward_std': 69.84455871582031, 'kl': 0.0017996366368606687, 'epoch': 0.09}\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[reward_len],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "wandb.init(project=\"GRPO\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = trainer.model.merge_and_unload()\n",
    "merged_model.push_to_hub(\n",
    "    \"SmolGRPO-135M\", private=False, tags=[\"GRPO\", \"Reasoning-Course\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "# A long document about the Cat\n",
    "\n",
    "The cat (Felis catus), also referred to as the domestic cat or house cat, is a small \n",
    "domesticated carnivorous mammal. It is the only domesticated species of the family Felidae.\n",
    "Advances in archaeology and genetics have shown that the domestication of the cat occurred\n",
    "in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges\n",
    "freely as a feral cat avoiding human contact. It is valued by humans for companionship and\n",
    "its ability to kill vermin. Its retractable claws are adapted to killing small prey species\n",
    "such as mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth,\n",
    "and its night vision and sense of smell are well developed. It is a social species,\n",
    "but a solitary hunter and a crepuscular predator. Cat communication includes\n",
    "vocalizations—including meowing, purring, trilling, hissing, growling, and grunting—as\n",
    "well as body language. It can hear sounds too faint or too high in frequency for human ears,\n",
    "such as those made by small mammals. It secretes and perceives pheromones.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "from transformers import pipeline\n",
    "\n",
    "#generator = pipeline(\"text-generation\", model=\"SmolGRPO-135M\")\n",
    "\n",
    "## Or use the model and tokenizer we defined earlier\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.5,\n",
    "    \"min_p\": 0.1,\n",
    "}\n",
    "\n",
    "generated_text = generator(messages, generate_kwargs=generate_kwargs)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
